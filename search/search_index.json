{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the SiriDB Enodo docs For sourcecode please visit the SiriDB Enodo Hub repo . About SiriDB Enodo Enodo is a time series data analysis platform made for SiriDB. Combining the power of storing and querying time series data from SiriDB and the analyzing power of Enodo, we can create better understanding of the data that we collect and store. So we can learn from the past and create forecasts for the future. The Enodo platform is build in modules to create scalability. The Hub will control and organize the data that we have collected and the questions we want to ask about the data. The Worker will perform the analyzing jobs and the Listener will stay on top of the latest data points. Together these components will make sure we can monitor the data in realtime, adjust our expectations for the future and watch for sudden unexpected changes in the data that we collect. Listener The Enodo listener listens to a pipe socket with SiriDB server. The listener only keeps track of series that are registered via the hub. It sums up the added data points to each of these series and it sends periodically an update to the hub, or when a serie is monitored in realtime, it will notify the Hub immediately. The listener is separated from the Enodo hub, so that it can be placed close to the SiriDB server to gain local access to the pipe socket. Worker Note : A worker uses significant CPU and thus should be placed on a machine that has low CPU usage. The Enodo worker executes fitting and forecasting models/algorithms. It can create different models like MA/RA/ARIMA, but also use Prophet, for analyzing series, training models with new data, flagging anomalies and calculate forecasts for a certain series. A worker can implement multiple models. This can be different per worker version. The implemented models should be communicated to the hub during the handshake. Hub The Enodo hub communicates and guides both the listener and the worker. It tells the listener to which series it needs to pay attention to, and it tells the worker which series should be analysed. Clients can connect to the hub for receiving updates, and polling for data. Also a client can use the hub to alter the data about which series should be watched.","title":"Welcome to the SiriDB Enodo docs"},{"location":"#welcome-to-the-siridb-enodo-docs","text":"For sourcecode please visit the SiriDB Enodo Hub repo .","title":"Welcome to the SiriDB Enodo docs"},{"location":"#about-siridb-enodo","text":"Enodo is a time series data analysis platform made for SiriDB. Combining the power of storing and querying time series data from SiriDB and the analyzing power of Enodo, we can create better understanding of the data that we collect and store. So we can learn from the past and create forecasts for the future. The Enodo platform is build in modules to create scalability. The Hub will control and organize the data that we have collected and the questions we want to ask about the data. The Worker will perform the analyzing jobs and the Listener will stay on top of the latest data points. Together these components will make sure we can monitor the data in realtime, adjust our expectations for the future and watch for sudden unexpected changes in the data that we collect. Listener The Enodo listener listens to a pipe socket with SiriDB server. The listener only keeps track of series that are registered via the hub. It sums up the added data points to each of these series and it sends periodically an update to the hub, or when a serie is monitored in realtime, it will notify the Hub immediately. The listener is separated from the Enodo hub, so that it can be placed close to the SiriDB server to gain local access to the pipe socket. Worker Note : A worker uses significant CPU and thus should be placed on a machine that has low CPU usage. The Enodo worker executes fitting and forecasting models/algorithms. It can create different models like MA/RA/ARIMA, but also use Prophet, for analyzing series, training models with new data, flagging anomalies and calculate forecasts for a certain series. A worker can implement multiple models. This can be different per worker version. The implemented models should be communicated to the hub during the handshake. Hub The Enodo hub communicates and guides both the listener and the worker. It tells the listener to which series it needs to pay attention to, and it tells the worker which series should be analysed. Clients can connect to the hub for receiving updates, and polling for data. Also a client can use the hub to alter the data about which series should be watched.","title":"About SiriDB Enodo"},{"location":"getting_started/","text":"Getting started Deployment There are just a few strict rules to take into account when deploying the enodo platform. The Listener should be placed right next to you siridb instance, so it can listen to incoming data via a pipe. The worker can be deployed anywhere, as long as there is enough CPU resource to be used by the worker. The Hub should be deployed in such a way that it both the listener and the worker can connect to it via socket connections. Docker TODO Localhost Follow these steps for all the Enodo components: Install dependencies via pip3 install -r requirements.txt Setup a .conf file file python3 main.py --create_config There will be made a default.conf next to the main.py. Fill in the default.conf file Call python3 main.py --config=default.conf to start the hub. You can also setup the config by environment variables. These names are identical to those in the default.conf file, except all uppercase. Follow these additional steps for the Enodo Hub: Fill in settings.enodo file, which you can find in the data folder by the path set in the conf file with key: enodo_base_save_path Restart Hub to use new settings or fill them in via the GUI Enodo Hub API The Enodo Hub has two API's from which you can do CRUD actions and subscribe to data changes. A REST API and a socket.io API. REST API Resource path CRUD /api/series CR /api/series/{series_name} RD /api/enodo/event/output CR api/enodo/event/output/{output_id} D Examples Create Series call {hostname}/api/series (POST) { \"name\": \"series_name_in_siridb\", \"config\": { \"min_data_points\":2, \"job_config\":{ \"job_base_analysis\": { \"activated\": true, \"model\": \"prophet\", \"job_schedule\": 200, \"model_params\":{ \"points_since\":1563723900, \"sensitivity\":2, \"static_rules\":{ \"min\":800, \"max\":1000, \"last_n_points\":100 } } }, \"job_forecast\": { \"activated\": true, \"model\": \"ffe\", \"job_schedule\": 200, \"model_params\":{ \"points_since\":1563723900, \"sensitivity\":2, \"static_rules\":{ \"min\":800, \"max\":1000, \"last_n_points\":100 } } } } } } Create event output stream call {hostname}/api/enodo/event/output (POST) { \"output_type\": 1, \"data\": { \"for_severities\": [\"warning\", \"error\"], \"url\": \"url_to_endpoint\", \"headers\": { \"authorization\": \"Basic abcdefghijklmnopqrstuvwxyz\" }, \"payload\": \"{\\n \\\"title\\\": \\\"{{event.title}}\\\",\\n \\\"body\\\": \\\"{{event.message}}\\\",\\n \\\"dateTime\\\": {{event.ts}},\\n \\\"severity\\\": \\\"{{event.severity}}\\\"\\n}\" } } Socket.IO Api (WebSockets) When sending payload in a request, use the data structure same as in the REST API calls, except the data will be wrapped in an object : {\"data\": ...} . Get series event: /api/series/create Get series Details event: /api/series/details Create series event: /api/series/create Delete series event: /api/series/delete Get all event output stream event: /api/event/output Create event output stream event: /api/event/output/create Delete event output stream event: /api/event/output/delete Analysis Enodo support the following analysis jobs: Base series analysis for series characteristics Forecasting Anomaly detection Statis rules Each analysis job is send by the Hub to a available worker. The worker uses the series config to determine which model/algorithm to use for executing the job. Different workers can have different models implemented, which they let the Hub know while connecting on startup. 1. Base series analysis This job is meant to gather series characteristics and simple data such as if the series has a trend of detectable seasonality in it. 2. Forecasting The forecasting job results in a forecast of the series. The worker will use a requested model to forecast the series into the future, using the data we already have of this series. The forecast can differ between models and config. You can forecast just 5 hours into the future, or 5 days and so one. Depending on the amount of data and the model used for this job, it can be a very extensive or simple forecast. 3. Anomaly detection Using a requested model/algorithm the worker will try to check if in the last n points, there were any anomalies within the data. The more suffisticated the model or algorithm, the more precise the worker can be. 4. Static rules For simple series, a static threshold will do. For now Enodo support a min and max threshold. Analysis models Models can be installed within the worker analyser \u251c\u2500\u2500 model \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 base.py \u2502 \u251c\u2500\u2500 ffe \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 bootstrap.py \u2502 \u2502 \u2514\u2500\u2500 ffe.py \u2502 \u2514\u2500\u2500 prophet \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 bootstrap.py \u2502 \u2514\u2500\u2500 prophet.py \u2514\u2500\u2500 util.py Within the model folder you will find: util.py holding the load function, to load all model classes models folder holding all installed analyser models When creating a new model, make sure that the modelclass itself extends the basemodel class in base.py, and that the bootstrap.py file will have a function get_model_class which will return the model class itself (not an instance)","title":"Getting started"},{"location":"getting_started/#getting-started","text":"","title":"Getting started"},{"location":"getting_started/#deployment","text":"There are just a few strict rules to take into account when deploying the enodo platform. The Listener should be placed right next to you siridb instance, so it can listen to incoming data via a pipe. The worker can be deployed anywhere, as long as there is enough CPU resource to be used by the worker. The Hub should be deployed in such a way that it both the listener and the worker can connect to it via socket connections.","title":"Deployment"},{"location":"getting_started/#docker","text":"TODO","title":"Docker"},{"location":"getting_started/#localhost","text":"Follow these steps for all the Enodo components: Install dependencies via pip3 install -r requirements.txt Setup a .conf file file python3 main.py --create_config There will be made a default.conf next to the main.py. Fill in the default.conf file Call python3 main.py --config=default.conf to start the hub. You can also setup the config by environment variables. These names are identical to those in the default.conf file, except all uppercase. Follow these additional steps for the Enodo Hub: Fill in settings.enodo file, which you can find in the data folder by the path set in the conf file with key: enodo_base_save_path Restart Hub to use new settings or fill them in via the GUI","title":"Localhost"},{"location":"getting_started/#enodo-hub-api","text":"The Enodo Hub has two API's from which you can do CRUD actions and subscribe to data changes. A REST API and a socket.io API.","title":"Enodo Hub API"},{"location":"getting_started/#rest-api","text":"Resource path CRUD /api/series CR /api/series/{series_name} RD /api/enodo/event/output CR api/enodo/event/output/{output_id} D","title":"REST API"},{"location":"getting_started/#examples","text":"Create Series call {hostname}/api/series (POST) { \"name\": \"series_name_in_siridb\", \"config\": { \"min_data_points\":2, \"job_config\":{ \"job_base_analysis\": { \"activated\": true, \"model\": \"prophet\", \"job_schedule\": 200, \"model_params\":{ \"points_since\":1563723900, \"sensitivity\":2, \"static_rules\":{ \"min\":800, \"max\":1000, \"last_n_points\":100 } } }, \"job_forecast\": { \"activated\": true, \"model\": \"ffe\", \"job_schedule\": 200, \"model_params\":{ \"points_since\":1563723900, \"sensitivity\":2, \"static_rules\":{ \"min\":800, \"max\":1000, \"last_n_points\":100 } } } } } } Create event output stream call {hostname}/api/enodo/event/output (POST) { \"output_type\": 1, \"data\": { \"for_severities\": [\"warning\", \"error\"], \"url\": \"url_to_endpoint\", \"headers\": { \"authorization\": \"Basic abcdefghijklmnopqrstuvwxyz\" }, \"payload\": \"{\\n \\\"title\\\": \\\"{{event.title}}\\\",\\n \\\"body\\\": \\\"{{event.message}}\\\",\\n \\\"dateTime\\\": {{event.ts}},\\n \\\"severity\\\": \\\"{{event.severity}}\\\"\\n}\" } }","title":"Examples"},{"location":"getting_started/#socketio-api-websockets","text":"When sending payload in a request, use the data structure same as in the REST API calls, except the data will be wrapped in an object : {\"data\": ...} .","title":"Socket.IO Api (WebSockets)"},{"location":"getting_started/#get-series","text":"event: /api/series/create","title":"Get series"},{"location":"getting_started/#get-series-details","text":"event: /api/series/details","title":"Get series Details"},{"location":"getting_started/#create-series","text":"event: /api/series/create","title":"Create series"},{"location":"getting_started/#delete-series","text":"event: /api/series/delete","title":"Delete series"},{"location":"getting_started/#get-all-event-output-stream","text":"event: /api/event/output","title":"Get all event output stream"},{"location":"getting_started/#create-event-output-stream","text":"event: /api/event/output/create","title":"Create event output stream"},{"location":"getting_started/#delete-event-output-stream","text":"event: /api/event/output/delete","title":"Delete event output stream"},{"location":"getting_started/#analysis","text":"Enodo support the following analysis jobs: Base series analysis for series characteristics Forecasting Anomaly detection Statis rules Each analysis job is send by the Hub to a available worker. The worker uses the series config to determine which model/algorithm to use for executing the job. Different workers can have different models implemented, which they let the Hub know while connecting on startup.","title":"Analysis"},{"location":"getting_started/#1-base-series-analysis","text":"This job is meant to gather series characteristics and simple data such as if the series has a trend of detectable seasonality in it.","title":"1. Base series analysis"},{"location":"getting_started/#2-forecasting","text":"The forecasting job results in a forecast of the series. The worker will use a requested model to forecast the series into the future, using the data we already have of this series. The forecast can differ between models and config. You can forecast just 5 hours into the future, or 5 days and so one. Depending on the amount of data and the model used for this job, it can be a very extensive or simple forecast.","title":"2. Forecasting"},{"location":"getting_started/#3-anomaly-detection","text":"Using a requested model/algorithm the worker will try to check if in the last n points, there were any anomalies within the data. The more suffisticated the model or algorithm, the more precise the worker can be.","title":"3. Anomaly detection"},{"location":"getting_started/#4-static-rules","text":"For simple series, a static threshold will do. For now Enodo support a min and max threshold.","title":"4. Static rules"},{"location":"getting_started/#analysis-models","text":"Models can be installed within the worker analyser \u251c\u2500\u2500 model \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 base.py \u2502 \u251c\u2500\u2500 ffe \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 bootstrap.py \u2502 \u2502 \u2514\u2500\u2500 ffe.py \u2502 \u2514\u2500\u2500 prophet \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 bootstrap.py \u2502 \u2514\u2500\u2500 prophet.py \u2514\u2500\u2500 util.py Within the model folder you will find: util.py holding the load function, to load all model classes models folder holding all installed analyser models When creating a new model, make sure that the modelclass itself extends the basemodel class in base.py, and that the bootstrap.py file will have a function get_model_class which will return the model class itself (not an instance)","title":"Analysis models"}]}